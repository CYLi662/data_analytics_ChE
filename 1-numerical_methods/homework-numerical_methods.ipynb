{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#5-points:-Orthogonalization-of-Vectors\" data-toc-modified-id=\"5-points:-Orthogonalization-of-Vectors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>5 points: Orthogonalization of Vectors</a></span></li><li><span><a href=\"#10-points:-General-Linear-Regression-(3-peaks)\" data-toc-modified-id=\"10-points:-General-Linear-Regression-(3-peaks)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>10 points: General Linear Regression (3 peaks)</a></span></li><li><span><a href=\"#15-points:-General-Linear-Regression-(N-peaks)\" data-toc-modified-id=\"15-points:-General-Linear-Regression-(N-peaks)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>15 points: General Linear Regression (N peaks)</a></span></li><li><span><a href=\"#10-points:-Automatic-Differentiation\" data-toc-modified-id=\"10-points:-Automatic-Differentiation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>10 points: Automatic Differentiation</a></span></li><li><span><a href=\"#15-points:-Non-linear-Regression\" data-toc-modified-id=\"15-points:-Non-linear-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>15 points: Non-linear Regression</a></span></li><li><span><a href=\"#10-points-(6745-only):-Legendre-Polynomials\" data-toc-modified-id=\"10-points-(6745-only):-Legendre-Polynomials-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>10 points (6745 only): Legendre Polynomials</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Numerical Methods\n",
    "\n",
    "This homework will explore linear algebra, linear regression, and numerical optimization from the perspective of the ethanol IR spectra. The block below loads the data in and selects the region you will work with for this homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/ethanol_IR.csv')\n",
    "x_all = df['wavenumber [cm^-1]'].values\n",
    "y_all = df['absorbance'].values\n",
    "\n",
    "x_peak = x_all[100:250]\n",
    "y_peak = y_all[100:250]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak,y_peak, '-b', marker='.')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 points: Orthogonalization of Vectors\n",
    "\n",
    "Consider the vectors defined by two different sections of the spectra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_peak[0:50]\n",
    "y1 = y_peak[0:50]\n",
    "y2 = y_peak[50:100]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y1, '-b', marker='.')\n",
    "ax.plot(x,y2, '-r', marker='.')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an orthonormal basis from these 2 vectors. Plot the result and use inner products to show that the 2 vectors are orthonormal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 points: General Linear Regression (3 peaks)\n",
    "\n",
    "Assume there are 3 peaks of the same width in this region of the region of the spectra defined below, and that the peaks follow a Gaussian distribution. Use general linear regression to determine the best-fit under this assumption. Justify your answer by visually comparing the model to the data (i.e. plot the result of your regression model along with the original data) and *briefly* describing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_peak = x_all[100:250]\n",
    "y_peak = y_all[100:250]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak,y_peak, '-b', marker='.')\n",
    "ax.set_xlabel('wavenumber [cm^-1]')\n",
    "ax.set_ylabel('absorbance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 points: General Linear Regression (N peaks)\n",
    "\n",
    "Continue to work with the same region of the spectra, but assume you do not know how many peaks there are, or the widths of the peaks. However, you do know that they follow Gaussian distributions. Use your intuition and trial-and-error along with general linear regression to find a model that describes the data.\n",
    "\n",
    "For the purpose of this class, there is no right answer to the number/position/width of the peaks in the spectra (i.e. you don't need to review your spectroscopy textbook). As long as you justify your answer you will receive full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 points: Automatic Differentiation\n",
    "\n",
    "Now assume that you want to perform non-linear regression to improve the model by optimizing the positions and widths of the peaks you guessed using trial-and-error in the prior problem. Write a loss function to do this, and use `autograd` to compute the derivative of the loss function when all of the parameters are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 points: Non-linear Regression\n",
    "\n",
    "Use the loss function you constructed in the prior problem to find the optimal position and widths of the peaks you have assumed are present. Assume that no peak areas can be negative, which you may or may not need to enforce with a constraint. Compare the result to the original data and the result you found with general linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 points (6745 only): Legendre Polynomials\n",
    "\n",
    "The \"Legendre Polynomials\" are an orthonormal set of polynomials defined on the domain [-1, 1]. Starting from the Vandermonde matrix, derive the first 4 (e.g. to order $x^3$) Legendre polynomials. You may do this numerically or analytically. If you choose to do it numerically you should verify your answers with the analytical solutions.\n",
    "\n",
    "Hint: You may want to use \"Gram-Schmidt\" orthogonalization to achieve this. It is discussed in Lecture 7 of Trefethen & Bau, as well as multiple sources online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
