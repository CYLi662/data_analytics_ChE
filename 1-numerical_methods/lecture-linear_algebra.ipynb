{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Matrix-vector-multiplication\" data-toc-modified-id=\"Matrix-vector-multiplication-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Matrix-vector multiplication</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Write-a-function-that-uses-for-loops-to-multiply-a-matrix-and-a-vector.\" data-toc-modified-id=\"Exercise:-Write-a-function-that-uses-for-loops-to-multiply-a-matrix-and-a-vector.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Exercise: Write a function that uses for loops to multiply a matrix and a vector.</a></span></li><li><span><a href=\"#Exercise:-Create-your-own-Vandermonde-matrix\" data-toc-modified-id=\"Exercise:-Create-your-own-Vandermonde-matrix-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Exercise: Create your own Vandermonde matrix</a></span></li></ul></li><li><span><a href=\"#Norms,-inner-products,-and-orthogonality\" data-toc-modified-id=\"Norms,-inner-products,-and-orthogonality-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Norms, inner products, and orthogonality</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Create-an-orthonormal-version-of-the-Vandermonde-matrix\" data-toc-modified-id=\"Exercise:-Create-an-orthonormal-version-of-the-Vandermonde-matrix-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercise: Create an orthonormal version of the Vandermonde matrix</a></span></li></ul></li><li><span><a href=\"#Rank,-Inverses,-and-Linear-Systems\" data-toc-modified-id=\"Rank,-Inverses,-and-Linear-Systems-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Rank, Inverses, and Linear Systems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-See-how-the-rank-changes-if-redundant-equations-are-selected.\" data-toc-modified-id=\"Exercise:-See-how-the-rank-changes-if-redundant-equations-are-selected.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Exercise: See how the rank changes if redundant equations are selected.</a></span></li></ul></li><li><span><a href=\"#Eigen-and-Singular-Value-Decompositions\" data-toc-modified-id=\"Eigen-and-Singular-Value-Decompositions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Eigen and Singular Value Decompositions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "Linear algebra is required for all engineers, but the conceptual aspects are often not taught or have been forgotten, so it is useful to have a refresher on some key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector multiplication\n",
    "\n",
    "First, some definitions:\n",
    "\n",
    "* Dot product or \"inner product\":\n",
    "\n",
    "$\\vec{a} \\cdot{} \\vec{b} = \\sum_i a_i b_i$\n",
    "\n",
    "* Matrix/vector multiplication:\n",
    "\n",
    "$\\bar{\\bar{A}} \\vec{x} = \\sum_j A_{ij} x_j$\n",
    "\n",
    "* Matrix/matrix multiplication:\n",
    "\n",
    "$\\bar{\\bar{A}} \\bar{\\bar{B}} = \\sum_j A_{ij} B_{jk}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write a function that uses for loops to multiply a matrix and a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore matrix-vector multiplication conceptually by constructing a dataset from the \"Vandermonde\" matrix of polynomials and a weight vector, $\\vec{w}$ to construct a dataset of the form:\n",
    "\n",
    "$y_i = w_0 + w_1 x_i + w_2 x_i^2$\n",
    "\n",
    "it may be useful to write this with summation notation and compare it to the matrix-vector multiplication definition:\n",
    "\n",
    "$y_i = \\sum_{j=0}^2 w_j x_i^{\\: j}$\n",
    "\n",
    "First, we can use `numpy` to create a vector $x_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "xi = np.linspace(0,10,11)\n",
    "xi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a new vector $z_i = x_i^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = xi**2\n",
    "zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a vector that contains each of the weight parameters, $w_j$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wj = [1.5, 0.8, -0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct $y_i$ manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = wj[0] + wj[1]*xi + wj[2]*zi\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(xi, yi, '--or')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but we can create the same dataset using a Vandermonde matrix, which is a matrix of polynomials defined as:\n",
    "\n",
    "$X_{ij} = x_i^{\\: j}$\n",
    "\n",
    "In other words, each column of the matrix consists of a different polynomial.\n",
    "\n",
    "We can construct this matrix using `numpy`. First, we need to turn $x_i$ into a column vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_i vector shape: {}\".format(xi.shape))\n",
    "xi_col = xi.reshape((xi.shape[0], 1)) #<- here we \"reshape\" the matrix into a column\n",
    "#xi_col = xi.reshape(-1, 1) #<- this is equivalent, but less clear. It is a common shortcut.\n",
    "print(\"x_i column shape: {}\".format(xi_col.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can \"stack\" these vectors together to create the Vandermonde matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = xi_col\n",
    "X_vdm = np.hstack((xi**0, xi**1, xi**2))\n",
    "X_vdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can directly create $y_i$ using matrix-vector multiplication based on the definition of matrix-vector multiplication:\n",
    "\n",
    "$\\bar{\\bar{X}}\\vec{w} = \\sum_j X_{ij}w_j $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi_vdm = X_vdm@wj\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(xi, yi_vdm, '--or')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that they are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi == yi_vdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is also useful to use `isclose` instead of `==` since numerical methods are prone to very small errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(yi, yi_vdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.all()` method is a good way to confirm that all values are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(yi, yi_vdm).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create your own Vandermonde matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms, inner products, and orthogonality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors can be described by various \"norms\" that capture their size and distance. The most common is the $L_2$ norm, also called the \"Euclidean distance\" which is defined as:\n",
    "\n",
    "$||\\vec{x}||_2 = \\sqrt{\\sum_i x_i^2}$\n",
    "\n",
    "This can also be computed by taking the square root of the vector-vector \"inner product\" of a vector with itself:\n",
    "\n",
    "$||\\vec{x}||_2 = \\sqrt{\\vec{x}^T \\vec{x}}$\n",
    "\n",
    "A vector is called \"normal\" if it's norm is 1. We can always \"normalize\" a vector by dividing it by its norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_0 = X_vdm[:,0]\n",
    "norm_col_0 = np.linalg.norm(col_0,2)\n",
    "col_0_normed = col_0/norm_col_0\n",
    "print('Column norm: {}'.format(norm_col_0))\n",
    "print('Normed column norm: {}'.format(np.linalg.norm(col_0_normed,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also occasionally use the $L_1$ norm, defined as:\n",
    "\n",
    "$||\\vec{x}||_1 = \\sum_i |x_i|$\n",
    "\n",
    "These norms, and others are discussed in detail in the online notes of [Machine Learning Refined](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/16_Linear_algebra/16_5_Norms.ipynb) and Lecture 3 of Trefethen & Bau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to remember that the inner product between two different vectors is equal to the product of their magnitudes and the cosine of the angle between them:\n",
    "\n",
    "$\\vec{x}'\\vec{y} = ||\\vec{x}||_2 ||\\vec{y}||_2 cos(\\theta)$\n",
    "\n",
    "This is described in detail in the online [Machine Learning Refined](https://github.com/jermwatt/machine_learning_refined/blob/gh-pages/notes/16_Linear_algebra/16_2_Vectors.ipynb) notes.\n",
    "\n",
    "We can use this to compute the angle between two normed vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1 = X_vdm[:,1]\n",
    "norm_col_1 = np.linalg.norm(col_1,2)\n",
    "col_1_normed = col_1/norm_col_1\n",
    "\n",
    "cos_theta = np.dot(col_1_normed, col_0_normed)\n",
    "theta = np.degrees(np.arccos(cos_theta))\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are defined as \"orthogonal\" if their \"inner product\" is zero. We can check that this is consistent with our typical definition of \"orthogonal\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.degrees(np.arccos(0))\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, orthogonal vectors are at right angles to each other, or have no projection onto each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key concept that comes in handy is the ability to find the orthogonal components of an arbitrary set of vectors. We can do this by subtracting off the projection of one vector onto another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1_ortho = col_1_normed - np.dot(col_0_normed, col_1_normed)*col_0_normed\n",
    "\n",
    "np.dot(col_1_ortho, col_0_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this isn't technically zero, it is very close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(np.dot(col_1_ortho, col_0_normed), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a discussion and proof of why this works for orthnormal vectors in Lecture 2 of Trefethen & Bau.\n",
    "\n",
    "Let's take a look at the original vectors as compared to the orthonormal ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1, figsize=(10,6))\n",
    "\n",
    "axes[0].plot(xi, col_0_normed, '--or')\n",
    "axes[0].plot(xi, col_1_normed, '--ob')\n",
    "\n",
    "axes[1].plot(xi, col_0_normed, '--or')\n",
    "axes[1].plot(xi, col_1_ortho, '--ob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these vectors don't appear to be at \"right angles\" to each other. Remember that these are not 2-dimensional vectors, but rather 10-dimensional vectors. Our intuition only goes so far when working in high-dimensional spaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create an orthonormal version of the Vandermonde matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank, Inverses, and Linear Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of the \"rank\" of a matrix is important. The formal definition of rank is the \"number of linearly independent columns/rows\". For an $m \\times n$ matrix, the rank is always less than or equal to the minimum of $m$ and $n$:\n",
    "\n",
    "rank $\\leq min(m, n)$\n",
    "\n",
    "It is sometimes convenient to think of rank in terms of a linear system of equations defined by:\n",
    "\n",
    "$\\bar{\\bar{A}}\\vec{x} = \\vec{b}$\n",
    "\n",
    "where $\\bar{\\bar{A}}$ is an $m \\times n$ matrix that defines equations in terms of unknown variables defined by $\\vec{x}$. If $m = n$ then $\\bar{\\bar{A}}$ is square and the number of equations is equal to the number of unknowns. As long as there are no redundant equations, then the rank of $\\bar{\\bar{A}}$ is equal to $m$ and $n$. However, if there are redundant equations then the rank is equal to the number of non-redundant equations and the system is **underconstrained**. On the other hand, if $m > n$ then there are more equations than unknowns, and the system is **overconstrained** (assuming there are no redundant equations).\n",
    "\n",
    "A matrix is **invertible** if and only if it is a square, full-rank matrix. This is equivalent to saying that a system of equations can only be solved ($\\vec{x} = \\bar{\\bar{A}}^{-1}\\vec{b}$) if the number of equations is equal to the number of unknowns (square matrix) and no equations are redundant (full-rank).\n",
    "\n",
    "Let's look at an example based on the Vandermonde matrix and our dataset from before. We will use 3 points to extract the weights, $w_j$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_vdm[:3, :]\n",
    "b = yi[:3]\n",
    "\n",
    "print('Shape of A: {}'.format(A.shape))\n",
    "print('Rank of A: {}'.format(np.linalg.matrix_rank(A)))\n",
    "\n",
    "A_inv = np.linalg.inv(A)\n",
    "w = A_inv@b\n",
    "\n",
    "print('Weights: {}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: See how the rank changes if redundant equations are selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is very inefficient to solve systems of equations with matrix inverses. You should be familiar with [Gaussian Elimination](http://mathworld.wolfram.com/GaussianElimination.html) from your linear algebra course. There are many other ways to solve linear systems, such as the QR factorization or using eigenvalues. Unfortunately we don't have time to cover these methods in this course, and will simply solve systems using the `solve` function from `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_solve = np.linalg.solve(A,b)\n",
    "np.isclose(w_solve, w).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, solving systems of equations is easy as long as (1) you can write the system in the form $\\bar{\\bar{A}}\\vec{x} = \\vec{b}$, and (2) the matrix $\\bar{\\bar{A}}$ is invertible (i.e. the system can be solved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen and Singular Value Decompositions\n",
    "\n",
    "The eigenvalue problem for a matrix $\\bar{\\bar{A}}$:\n",
    "\n",
    "$\\bar{\\bar{A}} v_n = \\lambda_n v_n$\n",
    "\n",
    "where $v_n$ is the $n$th eigenvector and $\\lambda_n$ is the $n$th eigenvalue.\n",
    "\n",
    "To calculate eigenvalues of a matrix, use the `eigvals` function, and for calculating both eigenvalues and eigenvectors, use the function `eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eigvals, eig\n",
    "\n",
    "print('Eigenvalues of A: {}'.format(eigvals(A)))\n",
    "\n",
    "vals, vecs = eig(A)\n",
    "print('Eigenvectors of A: {}'.format(vecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors of a **symmetric** matrix will always be orthonormal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sym = (A.T + A)/2. #make A symmetric\n",
    "vals, vecs = eig(A_sym)\n",
    "\n",
    "vec0 = vecs[:,0]\n",
    "vec1 = vecs[:,1]\n",
    "\n",
    "np.dot(vec0, vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigendecomposition is only possible for a square matrix. However, there is a similar concept called a \"singular value decomposition\", or SVD, that will work for any matrix:\n",
    "\n",
    "$A = \\hat{U}\\hat{\\Sigma}V^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "vecsL, vals, vecsR = svd(A_sym)\n",
    "np.isclose(vecsR[0,:], vecs[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a square matrix, the SVD is equivalent to the eigendecomposition, although the order of the vectors will not always be the same. In general, eigenvalues and singular values are ordered from largest to smallest, but this is not guaranteed by the algorithms that compute them.\n",
    "\n",
    "The advantage of the SVD is that it will also work for non-square matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsL, vals, vecsR = svd(X_vdm)\n",
    "\n",
    "print('Original matrix shape: {}'.format(X_vdm.shape))\n",
    "print('Left singular vectors shape: {}'.format(vecsL.shape))\n",
    "print('Right singular vectors shape: {}'.format(vecsR.shape))\n",
    "print('Singular Values: {}'.format(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more detailed discussion of the SVD in Lecture 4 of Trefethen & Bau. We will not study it directly in this course, but it is an important part of some algorithms that we will use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
