{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#PCA-and-Variance-(15-points)\" data-toc-modified-id=\"PCA-and-Variance-(15-points)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>PCA and Variance (15 points)</a></span></li><li><span><a href=\"#Linearized-Arrhenius-Regression-(10-points)\" data-toc-modified-id=\"Linearized-Arrhenius-Regression-(10-points)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linearized Arrhenius Regression (10 points)</a></span></li><li><span><a href=\"#Clustering-in-PCA-vs.-LDA-(15-points)\" data-toc-modified-id=\"Clustering-in-PCA-vs.-LDA-(15-points)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clustering in PCA vs. LDA (15 points)</a></span></li><li><span><a href=\"#One-Hot-Encoding-(5-points)\" data-toc-modified-id=\"One-Hot-Encoding-(5-points)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>One-Hot Encoding (5 points)</a></span></li><li><span><a href=\"#Symbolic-Classification-(15-points)\" data-toc-modified-id=\"Symbolic-Classification-(15-points)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Symbolic Classification (15 points)</a></span></li><li><span><a href=\"#More-to-come...\" data-toc-modified-id=\"More-to-come...-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>More to come...</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Feature Engineering Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Variance (15 points)\n",
    "(Adapted from Machine Learning Refined, Exercise 9.3)\n",
    "\n",
    "Consider a mean-centered dataset ($\\mu=0$), $\\bar{\\bar{X}}$, consisting of $N$ data points $\\vec{x}_i$ of dimension M (there are $M$ columns in $X_{ij}$). The variance of points projected along a specific unit direction, $\\vec{d}, can be given as:\n",
    "\n",
    "$$ \\sigma^2_d = \\frac{1}{N} \\sum_{i=0}^{N-1} (\\vec{x}_i \\cdot \\vec{d})^2 $$\n",
    "\n",
    "where $\\vec{x}_i \\cdot \\vec{d}$ is the dot product, or projection, of data point $x_i$ onto the arbitrary unit vector $\\vec{d}$.\n",
    "\n",
    "(a, 5pts) Show that $\\sigma^2_d$ is equivalent to $\\frac{1}{N} \\vec{d} \\bar{\\bar{X}}^T \\bar{\\bar{X}} \\vec{d}^T$\n",
    "\n",
    "*Hint: Expand into index notation, and remember that $\\sigma^2_d$ is a scalar*\n",
    "\n",
    "(b, 10pts) Compute the direction of maximum variance of the data.\n",
    "\n",
    "*Hint: Use the eigendecomposition of $\\bar{\\bar{X}}^T \\bar{\\bar{X}}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linearized Arrhenius Regression (10 points)\n",
    "\n",
    "Assume you are given rate data, $r_i$, as a function of temperatures, $T_i$, and you assume that the rate is determined by the Arrhenius expression:\n",
    "\n",
    "$r_i = A exp(\\frac{-G}{kT_i})$\n",
    "\n",
    "(a, 5pts) Use non-linear transformations to express this as a linear regression problem, $y_i = mx_i + b + \\epsilon_i$. Do you expect the residuals, $\\epsilon_i$ to be normally distributed? Briefly explain.\n",
    "\n",
    "(b, 5pts) Would the solution from (a) be found by a standard symbolic regression analysis ($r_i$ is not transformed)? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in PCA vs. LDA (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA and LDA to create a 9-dimensional representation of the MNIST dataset. Apply $k-means$ clustering with $k=10$ to both datasets. Assign a label to each cluster by using the most common label from the cluster, and create a confusion matrix based on the resulting clusters. Briefly explain the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset consisting of a single categorical variable with a total of $N$ datapoints and corresponding outputs, $y$. Assume that each datapoint has a unique category label such that there are $N$ distinct categories. You apply one-hot encoding to create a new feature matrix, $\\bar{\\bar{X}}$ and decide to fit a linear regression model:\n",
    "\n",
    "$\\vec{y} = \\bar{\\bar{X}}\\vec{w}$\n",
    "\n",
    "What will the optimal values of $\\vec{w}$ be and the $R^2$ be? Is this a reliable model? Briefly explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Classification (15 points)\n",
    "\n",
    "Use the `autofeat` package to create a classification model for the perovskite dataset. You should **not** use *t* or *tau* (the final 2 feature columns) as inputs. Identify the most important features, and compare your results to a logistic regression model based solely on *tau*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to come..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
