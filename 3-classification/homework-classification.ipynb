{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#(5-points)-Discrimination-boundary-for-quadratic-model\" data-toc-modified-id=\"(5-points)-Discrimination-boundary-for-quadratic-model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>(5 points) Discrimination boundary for quadratic model</a></span></li><li><span><a href=\"#(10-points)-Gradient-of-the-&quot;softmax&quot;-cost-function\" data-toc-modified-id=\"(10-points)-Gradient-of-the-&quot;softmax&quot;-cost-function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>(10 points) Gradient of the \"softmax\" cost function</a></span></li><li><span><a href=\"#(5-points)-Evaluating-a-margin-loss-model\" data-toc-modified-id=\"(5-points)-Evaluating-a-margin-loss-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>(5 points) Evaluating a margin loss model</a></span></li><li><span><a href=\"#(10-points)-Assessing-model-accuracy\" data-toc-modified-id=\"(10-points)-Assessing-model-accuracy-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>(10 points) Assessing model accuracy</a></span></li><li><span><a href=\"#(10-points)-Custom-non-linear-classification-boundaries\" data-toc-modified-id=\"(10-points)-Custom-non-linear-classification-boundaries-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>(10 points) Custom non-linear classification boundaries</a></span></li><li><span><a href=\"#(15-points)-Weighted-neigbhors-classification\" data-toc-modified-id=\"(15-points)-Weighted-neigbhors-classification-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>(15 points) Weighted neigbhors classification</a></span></li><li><span><a href=\"#(15-points)-Comparison-of-classification-model-types\" data-toc-modified-id=\"(15-points)-Comparison-of-classification-model-types-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>(15 points) Comparison of classification model types</a></span></li><li><span><a href=\"#(6745-only)-(10-points)-Logistic-regression-from-the-regression-perspective\" data-toc-modified-id=\"(6745-only)-(10-points)-Logistic-regression-from-the-regression-perspective-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>(6745 only) (10 points) Logistic regression from the regression perspective</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework will explore basic concepts in classification, including generalized linear models and support vector machines. In this homework you will work with the perovskite prediction dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets.samples_generator import make_blobs, make_moons, make_circles\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/perovskite_data.csv')\n",
    "X_perov = df[['nA', 'nB', 'nX', 'rA (Ang)', 'rB (Ang)', 'rX (Ang)', 't', 'tau']].values\n",
    "y_perov = df['exp_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 points) Discrimination boundary for quadratic model\n",
    "\n",
    "Consider a classification model defined by:\n",
    "\n",
    "$y_i = w_0 x_0 + w_1 x_1 + w_2 (x_0^2 + x_1^2)$\n",
    "\n",
    "where the model predicts class A if $y_i > 0$ and predicts class B if $y_i \\leq 0$. Derive the equation that defines the discrimination boundary. Briefly describe the nature of this boundary (linear/non-linear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Gradient of the \"softmax\" cost function\n",
    "\n",
    "The \"softmax\" loss function is defined as:\n",
    "\n",
    "$g(\\vec{w}) = \\sum_i log(1 + \\exp{(-y_i \\vec{x}_i^T \\vec{w}}))$\n",
    "\n",
    "where $\\vec{x}_i$ is the $i$th row of the input matrix $\\bar{\\bar{X}}$.\n",
    "\n",
    "Derive an analytical expression for the gradient (derivative) of the softmax function with respect to $\\vec{w}$, and use this to define a system of non-linear equations that could be solved to minimize the softmax loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5 points) Evaluating a margin loss model\n",
    "\n",
    "You have trained a generalized linear model using the \"margin\" loss function and obtained the following weight vector:\n",
    "\n",
    "$\\vec{w} = [0.23, 1.5, 2.1, 6.1]$\n",
    "\n",
    "Evaluate the model for an arbitrary input vector, $\\vec{x}_i$, of your choice. Assume that the first weight, $0.23$ corresponds to the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Assessing model accuracy\n",
    "\n",
    "Use the \"tau\" column of the perovskite dataset (`X_perov[:, -1]`) to train a logistic regression classification model. Plot the confusion matrix for the model, and report the accuracy, precision and recall. Consider a \"positive\" outcome to be defined by 1 (`y_perov=1`) and a negative outcome to be defined by -1 (`y_perov=-1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (10 points) Custom non-linear classification boundaries\n",
    "\n",
    "Consider the perovskite classification as a function of just 2 features, $r_A$ (`X_perov[:,3]`) and $r_B$ (`X_perov[:,4]`). First, build a \"baseline model\" based on logistic regression and report the accuracy and precision of the baseline model.\n",
    "\n",
    "Next, use a plot of the data (`y_perov` as a function of `X_perov[:,3]` and `X_perov[:,4]`) to determine a single \"custom\" feature that improves the separation performance as much as possible. The feature should be based only on $r_A$ and $r_B$. Plot the data as a function of this new feature, and report the accuracy and precision of a logistic regression model that includes $r_A$, $r_B$, and your feature. Briefly explain how you decided on the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (15 points) Weighted neigbhors classification\n",
    "\n",
    "Instead of selecting the k-nearest neighbors to vote, we could design an algorithm where all neighbors get to vote, but their vote is weighted inversely to their distance from the point of interest:\n",
    "\n",
    "$y_i = \\sum_j y_j/(||x_i - x_j||)$\n",
    "\n",
    "where $j$ is an index over all training points.\n",
    "\n",
    "Implement this algorithm and test it on the perovskite dataset using a random selection of 25% of the data as training data. Report it's accuracy and precision compared to a 5-nearest neigbhors algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (15 points) Comparison of classification model types\n",
    "\n",
    "Select at least 3 different classification models. These could be models discussed in the lectures, or others that you have learned about elsewhere. Compare the accuracy of these different models for the perovskite dataset.\n",
    "\n",
    "It is suggested that you first select a validation set using holdout (`train_test_split`), optimize at least one hyperparameter for each model using `GridSearchCV` on the training set, and finally evaluate the accuracy by predicting the results of the validation set. Briefly describe your conclusions based on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6745 only) (10 points) Logistic regression from the regression perspective\n",
    "\n",
    "An alternate interpretation of classification is that we are performing non-linear regression to fit a \"step function\" to our data. Since step functions are not differentiable (or, more accurately, since the derivative of a step function is 0 everywhere except the step), a smooth approximation with non-zero derivatives must be used. One such approximation is the \"tanh\" function:\n",
    "\n",
    "$\\tanh{(x)} = \\frac{2}{1 + \\exp{(-x)}} - 1$\n",
    "\n",
    "This leads to a reformulation of the classification problem as:\n",
    "\n",
    "$\\vec{y} = \\tanh{(\\bar{\\bar{X}}\\vec{w})}$\n",
    "\n",
    "Show that this is mathematically equivalent to \"logistic regression\", or minimization of the \"softmax\" cost function.\n",
    "\n",
    "*Hint:* You may want to review Ch. 4 of \"Machine Learning Refined\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
