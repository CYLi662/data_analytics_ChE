{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Structure-Continuum\" data-toc-modified-id=\"Data-Structure-Continuum-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Structure Continuum</a></span></li><li><span><a href=\"#Indexing-Data-in-Pandas\" data-toc-modified-id=\"Indexing-Data-in-Pandas-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Indexing Data in Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise:-Create-a-subset-of-the-Dow-dataset-containing-data-from-Dec.-5---Dec.-12,-2015\" data-toc-modified-id=\"Exercise:-Create-a-subset-of-the-Dow-dataset-containing-data-from-Dec.-5---Dec.-12,-2015-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercise: Create a subset of the Dow dataset containing data from Dec. 5 - Dec. 12, 2015</a></span></li></ul></li><li><span><a href=\"#Establishing-an-efficient-data-pipeline\" data-toc-modified-id=\"Establishing-an-efficient-data-pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Establishing an efficient data pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Raw-data-organization\" data-toc-modified-id=\"Raw-data-organization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Raw data organization</a></span></li><li><span><a href=\"#Metadata-storage\" data-toc-modified-id=\"Metadata-storage-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Metadata storage</a></span></li></ul></li><li><span><a href=\"#Missing-values\" data-toc-modified-id=\"Missing-values-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Missing values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-observations\" data-toc-modified-id=\"Dropping-observations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Dropping observations</a></span></li><li><span><a href=\"#Exercise:-Create-a-version-of-the-Dow-dataset-that-does-not-contain-expression-marks,-but-still-includes-blank/null-values\" data-toc-modified-id=\"Exercise:-Create-a-version-of-the-Dow-dataset-that-does-not-contain-expression-marks,-but-still-includes-blank/null-values-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Exercise: Create a version of the Dow dataset that does not contain expression marks, but still includes blank/null values</a></span></li><li><span><a href=\"#Dropping-features/variables\" data-toc-modified-id=\"Dropping-features/variables-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Dropping features/variables</a></span></li><li><span><a href=\"#Exercise:-Create-a-version-of-the-Dow-dataset-that-does-not-contain-the-Avg_Delta_Composition-Primary-Column-variable-and-does-not-contain-any-null-or-non-numeric-values\" data-toc-modified-id=\"Exercise:-Create-a-version-of-the-Dow-dataset-that-does-not-contain-the-Avg_Delta_Composition-Primary-Column-variable-and-does-not-contain-any-null-or-non-numeric-values-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Exercise: Create a version of the Dow dataset that does not contain the <code>Avg_Delta_Composition Primary Column</code> variable and does not contain any null or non-numeric values</a></span></li><li><span><a href=\"#Imputation\" data-toc-modified-id=\"Imputation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Imputation</a></span></li><li><span><a href=\"#Exercise:-Use-the-regression-model-above-to-&quot;impute&quot;-values-for-Avg_Delta_Composition-Primary-Column.\" data-toc-modified-id=\"Exercise:-Use-the-regression-model-above-to-&quot;impute&quot;-values-for-Avg_Delta_Composition-Primary-Column.-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Exercise: Use the regression model above to \"impute\" values for <code>Avg_Delta_Composition Primary Column</code>.</a></span></li></ul></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-normally-distributed-data\" data-toc-modified-id=\"Univariate-normally-distributed-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Univariate normally distributed data</a></span></li><li><span><a href=\"#Exercise:-Create-a-version-of-the-Dow-dataset-where-outliers-are-removed-by-assuming-each-variable-is-an-independent-and-normally-distributed-dataset\" data-toc-modified-id=\"Exercise:-Create-a-version-of-the-Dow-dataset-where-outliers-are-removed-by-assuming-each-variable-is-an-independent-and-normally-distributed-dataset-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Exercise: Create a version of the Dow dataset where outliers are removed by assuming each variable is an independent and normally distributed dataset</a></span></li><li><span><a href=\"#Discussion:-Which-variables-from-the-Dow-dataset-are-most/least-well-suited-for-the-z-score-approach?\" data-toc-modified-id=\"Discussion:-Which-variables-from-the-Dow-dataset-are-most/least-well-suited-for-the-z-score-approach?-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Discussion: Which variables from the Dow dataset are most/least well suited for the z-score approach?</a></span></li><li><span><a href=\"#Clustering-and-dimensional-reduction\" data-toc-modified-id=\"Clustering-and-dimensional-reduction-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Clustering and dimensional reduction</a></span></li></ul></li><li><span><a href=\"#Dealing-with-large-datasets\" data-toc-modified-id=\"Dealing-with-large-datasets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dealing with large datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#HDF-5-files\" data-toc-modified-id=\"HDF-5-files-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>HDF 5 files</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Organization\n",
    "\n",
    "## Data Structure Continuum\n",
    "\n",
    "Data (science) is like an iceberg:\n",
    "\n",
    "* Over 80% of time is spent cleaning/structuring data\n",
    "* Over 80% of existing data is \"unstructured\"\n",
    "* 90% of the data ever generated by humanity was generated in the last [two years](https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-the-mind-blowing-stats-everyone-should-read/#4f728b360ba9)!\n",
    "    - ...but how much of it is useful?\n",
    "\n",
    "<center>\n",
    "<img src=\"images/data-iceberg.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "In this course most data will be at least partially cleaned, but in the real world this is rarely the case.\n",
    "\n",
    "Most structured\n",
    "\n",
    "* Index data with integers (matrices)\n",
    "* **Index data with strings/integers (dataframes)**\n",
    "* Access data with structured queries (SQL)\n",
    "* **Access data through web requests (APIs)**\n",
    "* Access data with unstructured queries (search engines/no SQL)\n",
    "* Access data through parsers (scraping)\n",
    "\n",
    "\n",
    "Least structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "df.head(3) #<- shows the first 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data in Pandas\n",
    "\n",
    "Pandas provides [many options](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) for how to index/acces data.\n",
    "\n",
    "* square brackets `[]` index by column name\n",
    "* `.loc` method indexes by `[row_index]` or `[row_index, column]`\n",
    "* `.iloc` method indexes by `[row_number]` or `[row_number, column`\n",
    "* indexes can be lists or slices\n",
    "\n",
    "Columns/rows are returned as `Series` objects, which are basically 1-dimensional dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x1:Primary Column Reflux Flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]\n",
    "df.loc[0, 'x1:Primary Column Reflux Flow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be \"filtered\" using logical statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filtered = df[df['x1:Primary Column Reflux Flow'] > 350]\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "\n",
    "* The \"filtered\" dataset must be re-assigned to a new variable\n",
    "\n",
    "* The row indexes no longer start at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.loc[0]\n",
    "df_filtered.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns can be re-named to more convenient names by creating a renaming dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns\n",
    "#print(columns)\n",
    "new_columns = {}\n",
    "for ci in columns:\n",
    "    if ':' in ci:\n",
    "        new_columns[ci] = ci.split(':',1)[0]\n",
    "    else:\n",
    "        new_columns[ci] = ci\n",
    "print(new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shortnames = df.rename(columns=new_columns)\n",
    "df_shortnames.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can also be [indexed by date/time](https://towardsdatascience.com/basic-time-series-manipulation-with-pandas-4432afee64ea), which can be extremely convenient when working with time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_datetime = df.set_index('Date')\n",
    "df_datetime.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows indexing with slices between different times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_oneday = df_datetime['2015-12-02':'2015-12-03']\n",
    "df_oneday.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying structure of a `pandas` dataframe is a `numpy` array, which can be accessed with the `.values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_oneday = df_oneday.values\n",
    "X_oneday.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a subset of the Dow dataset containing data from Dec. 5 - Dec. 12, 2015\n",
    "\n",
    "The data should only include columns for `x1` to `x12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more useful features in `pandas`, far too many to cover in this course. Any time you are stuck it is a good idea to check the documentation or StackOverflow for a shortcut to get to the right \"slice\" of data in `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing an efficient data pipeline\n",
    "\n",
    "* Good integration of data storage + analysis can make or break a real project\n",
    "* Separate raw data from analysis facilitates reproducibility and scalability\n",
    "* Rapidly prototype and iteratevely improve.\n",
    "\n",
    "### Raw data organization\n",
    "\n",
    "Common issues with spreadsheets (from [Data Carpentry](https://datacarpentry.org/spreadsheet-ecology-lesson/02-common-mistakes/index.html)):\n",
    " * Using multiple tables\n",
    " * Using multiple tabs\n",
    " * Not filling in zeros\n",
    " * Using problematic null values\n",
    " * Using formatting to convey information\n",
    " * Using formatting to make the data sheet look pretty\n",
    " * Placing comments or units in cells\n",
    " * Entering more than one piece of information in a cell\n",
    " * Using problematic field names\n",
    " * Using special characters in data\n",
    " * Inclusion of metadata in data table\n",
    " * Date formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to convert the Dow dataset to numerical values to see if there are any null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nondate_cols = df.columns[1:]\n",
    "df.loc[:, nondate_cols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the data type (`dtype`) is `object` suggests that not all the values are numerical. We can look for null values using the `isnull` function of `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pd.isnull(df)\n",
    "df[pd.isnull(df).values].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one way to find all the null values. Another is to try to convert everything to numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X = df.loc[:, nondate_cols].values.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are some expression points in the data. This use of multiple types of \"null\" values can cause headaches. There is a very useful feature of `pandas` that allows us to apply an arbitrary function to data. Here, we can write a function that allows identification of only numerical values and apply it to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_real_and_finite(x):\n",
    "    if not np.isreal(x):\n",
    "        return False\n",
    "    elif not np.isfinite(x):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "numeric_map = df[nondate_cols].applymap(is_real_and_finite)\n",
    "numeric_map.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to identify missing or null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata storage\n",
    "\n",
    "* \"Metadata\" is data that describes the data. Without good metadata, data may become useless.\n",
    "\n",
    "* Metadata should always be stored in the same file as the data (typically as a \"header\").\n",
    "\n",
    "* Metadata organization should follow similar guidelines to data organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Missing values are extremely common in real data, and can cause problems. There are a few options for handling this. \n",
    "\n",
    "### Dropping observations\n",
    "\n",
    "The most common way of dealing with missing data is to simply drop observations. This works well if you have a large amount of data and it doesn't matter if you lose some. We can use the `.all` function across the row to see which rows contain non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_rows = numeric_map.all(axis=1)\n",
    "N_non_numeric = numeric_rows.tolist().count(False)\n",
    "print('Number of non-numeric values: {}'.format(N_non_numeric))\n",
    "N = numeric_map.shape[0]\n",
    "print('Proportion of non-numeric values: {:.2f}'.format(N_non_numeric/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of non-numeric values is very low, so dropping the observations is a reasonable strategy. We can use the numeric map to drop the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_rows = numeric_map.all(axis=1).values\n",
    "df_dropped_obs = df[real_rows]\n",
    "print(df_dropped_obs.shape)\n",
    "X = df_dropped_obs.values[:, 1:].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a version of the Dow dataset that does not contain expression marks, but still includes blank/null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping features/variables\n",
    "\n",
    "Another option is to drop features where there are a lot of missing values. This can be particularly useful if there are lot of features but fewer observations, or if the missing values occur in features that are not expected to contain much information.\n",
    "\n",
    "We can use the \"map\" of numeric values to find out which features have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_cols = numeric_map.all(axis=0)\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are at least some non-numeric values in all columns except for `y:Impurity`. We can also count the numer of missing values for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in numeric_map.columns:\n",
    "    print(col)\n",
    "    print(numeric_map[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most columns are only missing 20-30 values, except for `Avg_Delta_Composition Primary Column`, which is missing 291 values. We can check the correlation matrix to see if this value is correlated with any others. The `seaborn` library has a very nice function for plotting this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "corr = df.corr()\n",
    "sn.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr[\"Avg_Delta_Composition Primary Column\"] > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr[\"Avg_Delta_Composition Primary Column\"][corr[\"Avg_Delta_Composition Primary Column\"] > 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is highly correlated with the `Primary Column Make Flow` variable, which suggests that we will not lose much information if we drop this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a version of the Dow dataset that does not contain the `Avg_Delta_Composition Primary Column` variable and does not contain any null or non-numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "The final technique for dealing with missing data is \"imputation\", or trying to fill in missing values based on some approximation or relationship between known variables. Imputation is advantageous because you do not lose any data, but the tradeoff is that some assumptions or approximations must be made that can potentially bias or corrupt the data.\n",
    "\n",
    "This is also called a \"matrix completion\", and is [heavily studied](http://jmlr.org/papers/volume16/hastie15a/hastie15a.pdf) due to a [Netflix competition](https://en.wikipedia.org/wiki/Netflix_Prize) that awarded a million dollars for a solution to the problem of \"completing matrices\" that predicted what movies people would like. \n",
    "\n",
    "We won't go into dept on imputation, but as an example, consider the following linear regression model between `Primary Column Make Flow` and `Avg_Delta_Composition Primary Column`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = df_dropped_obs[\"x6:Primary Column Make Flow\"].values.reshape(-1,1)\n",
    "y = df_dropped_obs[\"Avg_Delta_Composition Primary Column\"].values\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x, y) \n",
    "r2 = model.score(x, y)\n",
    "print(r2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,y, 'ok', alpha=0.05)\n",
    "ax.plot(x, model.predict(x), '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use the regression model above to \"impute\" values for `Avg_Delta_Composition Primary Column`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outliers\n",
    "\n",
    "Real data almost always contains \"outliers\". An outlier is defined as an \"observation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism.\" ([Hawkins, 1980](https://link.springer.com/book/10.1007%2F978-94-015-3994-4)). Identifying outliers can be very difficult for complex data.\n",
    "\n",
    "There are two general types of approaches:\n",
    "\n",
    "* parametric: Assume a form of the underlying distribution, then remove data that have a sufficiently low probability of occuring given that distribution.\n",
    "\n",
    "* non-parameteric: Use distance metrics to identify points that are very far away from others.\n",
    "\n",
    "### Univariate normally distributed data\n",
    "\n",
    "The simplest case of outlier detection is to look for outliers in a 1-dimensional dataset that follows a normal distribution. This is typically done using a z-score:\n",
    "\n",
    "$z_i = \\frac{x_i-\\mu}{\\sigma}$\n",
    "\n",
    "where $x_i$ is the data point, $\\mu$ is the mean of the normal distribution, and $\\sigma$ is the standard deviation. This essentially tells us how many standard deviations away from the mean a data point is. Then, we can apply a cutoff, typically $z=3$ and drop any data that is outside the cutoff. A cutoff of $z=3$ is chosen since this corresponds to 99.7% of the data, so assuming the data actually follows the distribution we would only risk discarding 0.03% of real data points.\n",
    "\n",
    "Let's try it for a variable in the Dow dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = df_dropped_obs[\"x3:Input to Primary Column Bed 3 Flow\"].copy()\n",
    "mu = np.mean(xi)\n",
    "stdev = np.std(xi)\n",
    "z_cutoff = 3\n",
    "\n",
    "zi = (xi - mu)/stdev\n",
    "xi_nooutliers = xi[np.abs(zi) < z_cutoff]\n",
    "print(xi.shape)\n",
    "print(xi_nooutliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot these to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "\n",
    "axes[0].hist(xi.values)\n",
    "ylim = axes[0].get_ylim()\n",
    "axes[0].plot([mu, mu], ylim , '-k') #plot mean\n",
    "axes[0].plot([mu-z_cutoff*stdev, mu-z_cutoff*stdev], ylim, '-r') #plot stdev\n",
    "axes[0].plot([mu+z_cutoff*stdev, mu+z_cutoff*stdev], ylim, '-r') #plot stdev\n",
    "axes[1].hist(xi_nooutliers.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create a version of the Dow dataset where outliers are removed by assuming each variable is an independent and normally distributed dataset\n",
    "\n",
    "Use a z-score cutoff of 3 and assess what percentage of the data is identified as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has 2 major problems:\n",
    "\n",
    "1) It assumes that you have a 1-d dataset, or that all variables are independent.\n",
    "\n",
    "2) It assumes that data follows a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *parametric* approach, and works well under the given assumptions. However, these are often violated in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Which variables from the Dow dataset are most/least well suited for the z-score approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and dimensional reduction\n",
    "\n",
    "When working with high-dimensional data it becomes very difficult to identify outliers. There are two other classes of algorithms, clustering and dimensional reduction, that can be used. We will revisit this problem in later lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with large datasets\n",
    "\n",
    "The datasets we see in this class are relatively small (<1 million data points), but in real settings you may have much larger datasets that contain millions or even billions of data points. Or, you might have data that is very high-dimensional, with thousands or even millions of potential \"features\" for each observation (for example a 3-d image or simulation on a grid). For these situations spreadsheets become very inconvenient.\n",
    "\n",
    "Even for the spreadsheet we have been working with it takes a bit of time to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_reload = pd.read_excel('data/impurity_dataset-training.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few seconds may not seem like a lot, but if you have to re-load the data every time you test a script, and you test a script a few hundred times, this can add up to a significant amount of time lost. One trick is to work in Jupyter notebooks, where we can load the data once, then work with it to debug any analysis code.\n",
    "\n",
    "Another option is to switch to a different format. The HDF5 format is very well suited for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF 5 files\n",
    "\n",
    "HDF5 files (hierarchical data format) are useful for structured data, especially for very large datasets. HDF5 files act like mini filesystems, and have \"attributes\" to include additional context.\n",
    "\n",
    "We can create and manipulate HDF5 files from within Python using the `h5py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to delete any old versions of the file to avoid errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm data/impurity_data.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the Dow dataset into an HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"data/impurity_data.hdf5\", \"w\") #<- the \"w\" argument tells h5py to create a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the data values can be accessed with the `.values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dow = df_dropped_obs.values\n",
    "X_dow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of HDF5 is that it only works with numerical values. We can convert our data to floats if we drop the date column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dow_numbers = X_dow[:, 1:].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an HDF5 dataset and assign the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"training\", X_dow_numbers.shape)\n",
    "dset[:,:] = X_dow_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset will act like a `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(dset[0,0])\n",
    "print(X_dow_numbers[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 files are \"hierarchical\", so we can also create groups and add data as sub-groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = f.create_group('data_by_feature')\n",
    "x1 = grp.create_dataset('x1',dset[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1[:] = X_dow_numbers[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate through the data in an HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in f:\n",
    "    print(d, f[d])\n",
    "\n",
    "for d in f['data_by_feature']:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing about HDF5 files is that the object interacts directly with the file, and can be opened in multiple instances. This can be useful, but can also be confusing (similar to `numpy` arrays that aren't copied):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = h5py.File(\"data/impurity_data.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change data in one variable, it will change the file, and therefore change the data in the other instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2['training'][0,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f['training'][0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 files also enable storage of \"attributes\" that describe data. This can be very useful for meta-data storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['training'].attrs['name'] = \"Impurity training set\"\n",
    "f['training'].attrs['company'] = \"Dow Chemical\"\n",
    "f['training'].attrs['course'] = 'Data Analytics for Chemical Engineers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['training'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['training'].attrs['course']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing about HDF5 files is that when you are done working with them they need to be closed. Failure to do so can create errors, or even risk corrupting the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one of the greatest strengths of HDF5 files is that opening them does not read all of the data into temporary memory. This means that you can open very large files very quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f2 = h5py.File(\"data/impurity_data.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to seconds when loading in the corresponding Excel file. The tradeoff is that the data has to be loaded when you use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_loaded = f2['training'][:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time X_preloaded = X_dow_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is still much faster. The other advantage is that HDF5 *only reads in the subset you ask for*. For example, if we only want to read in a single column of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time X_col = f2['training'][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is only slightly faster, but if the dataset were very large (e.g. GB or TB) there would be a much bigger difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more features for HDF5 files. We won't cover them in lectures, but it is worth reading up on them if you are working with very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
